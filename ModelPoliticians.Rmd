---
title: "Model Politicians"
output: html_document
---
```{r setup, include=FALSE}
library(plyr)     # for recoding data
library(NLP)      # for NLP
library(tm)       # for Corpus()
library(e1071)    # svm
library(rpart)    # decision trees
library(ada)      # for adaboost
library(lsa)      # LSA
library(Matrix)   # for SVM matricies
library(SparseM)  # for SVM matricies
library(caret)    # for Precision, Recall()
library(knitr)    # for kable
```

## Helper Functions
```{r Helper}
load.data.politicians <- function() {
  # preprocessed in python
  tweets = read.csv('tweets_by_politicians.csv') # data is preprocessed in python
  tweets = tweets[,-1]
  tweets[1:3,]
  table(tweets$party) # D:9, R:9
  
  tweets$text = as.character(tweets$text)
  
  # Recode Dems = 0, Reps = 1
  tweets$party = revalue(tweets$party, c("D"=0, "R"=1))

  tweets[1:3,]
  return(tweets)
}

load.data.DR <- function() {
  # preprocessed in python
  tweets = read.csv('DR_users.csv') # data is preprocessed in python
  tweets = tweets[,-1]
  tweets[1:3,]
  table(tweets$party) # D:881, R:383
  
  tweets$text = as.character(tweets$text)
  tweets$state_code = as.character(tweets$state_code)
  
  # Recode Dems = 0, Reps = 1
  tweets$party = revalue(tweets$party, c("D"=0, "R"=1))
  any(is.na(tweets$party))

  tweets[1:3,]
  return(tweets)
}



do.lsa <- function(dtm.tfidf, ndims=10) {
  S = svd(dtm.tfidf, nu = ndims, nv = ndims)
  dtm.lsa = S$u
  return(dtm.lsa)
}

do.pca <- function(dtm) {
  pca = prcomp(dtm, scale=FALSE)
  dtm.pca = predict(pca) # 1264, 1264
  return(dtm.pca)
}


# Return reusable model of training data
model.fit <- function(X_train, y_train, classifier) {
   switch(classifier, 
        nb = {
          model = naiveBayes(x=X_train, y=y_train)
          # y_predictions = predict(model, newdata=X_test)
         },
        svmradial = {
          X_train = as.matrix.csr(X_train)  # 1,244
          # X_test  = as.matrix.csr(X_test)    # 622
          model = svm(X_train, y_train, kernel="radial")
          # y_predictions = predict(model, newdata=X_test)
        },
        svmlinear = {
          X_train = as.matrix.csr(X_train)  # 1,244
          # X_test  = as.matrix.csr(X_test)    # 622
          model = svm(X_train, y_train, kernel="linear")
          # y_predictions = predict(model, newdata=X_test)
        }
   )
  return(model)
}

# Return predictions of y_test
model.predict <- function(X_test, model) {
  # X_test = as.data.frame(X_test)
  y_predictions = predict(model, newdata=X_test)
  return(y_predictions)
}

# Find Precision, Recall, and F Score
model.evaluate <- function(y_predictions, y_test) {
  Precision = posPredValue(y_predictions, y_test)
  Recall    = sensitivity(y_predictions, y_test)
  F1 = (2 * Precision * Recall) / (Precision + Recall)

  # bind values together
  col = rbind(Precision, Recall, F1)
  return(col)
}

# Cross Validate
crossValidate <- function(document_term_matrix, y, classifier, k.fold=3) {
  eval = matrix(, nrow = 3, ncol = 0) # empty matrix with 3 rows
  n.obs = nrow(document_term_matrix) # no. of observations 
  s = sample(n.obs)
  # k = 1
  # k.fold = 3
  for (k in 1:k.fold) {
    test.idx = which(s %% k.fold == (k-1) ) # use modular operator
    X_train = as.matrix(document_term_matrix[-test.idx,])  # 1,244
    X_test  = as.matrix(document_term_matrix[test.idx,])    # 622
    y_train = y[-test.idx]                      # 1,244
    y_test  = y[test.idx]                        # 622
    # model 
    model = model.fit(X_train, y_train, classifier)
    y_predictions = model.predict(X_test, model)
    # evaluate predictions -- obtain precision, recall, F1 score
    k.eval = model.evaluate(y_predictions, y_test)
    eval = cbind(eval, k.eval)
  }
  
  # return table of K fold precision, recall, fscore 
  colnames(eval) = c("Fold 1", "Fold 2", "Fold 3")
  eval
  means = rowMeans(eval)
  eval = cbind(eval, Mean=means)
  eval
    
  return(eval)
}

```

## Load Data & Create DTMs --> Need to find intersection between User & Politician Terms
```{r message=FALSE}
poli.tweets = load.data.politicians()
user.tweets = load.data.DR()

# Get corpus
poli.corpus = Corpus(VectorSource(poli.tweets$text))
user.corpus = Corpus(VectorSource(user.tweets$text))

user.document_term_matrix = DocumentTermMatrix(user.corpus)
# Get DTM & y, and TFIDF Dtm & y.tfidf using user.corpus as a dictionary 
## http://stackoverflow.com/questions/16630627/how-to-recreate-same-documenttermmatrix-with-new-test-data
document_term_matrix = DocumentTermMatrix(poli.corpus, control = list(dictionary=Terms(user.document_term_matrix))) 
# 18 rows, 9137 cols (terms used), (11897 w/user dtm dictionary)
y = poli.tweets$party # 18

dtm.tfidf = DocumentTermMatrix(poli.corpus, control = list(weighting=weightTfIdf, dictionary=Terms(user.document_term_matrix))) # no empty rows
y.tfidf = poli.tweets$party

# Get reduced DTMs
## PCA and LSA
dtm.lsa.10 = do.lsa(dtm.tfidf, ndims=10) # 1264, 10; Matrix object
dtm.pca = do.pca(document_term_matrix) # 1264, 1264; Matrix object

dim(dtm.lsa.10)
dim(dtm.pca)
```

## Build Model on Politician's Twitter Data
## Run Cross Validation
#### Features: Standard DTM, TFIDF DTM, LSA DTM, and PCA DTM
#### Classifiers: Naive Bayes, SVM-Radial Kernel, SVM-Linear Kernel

### Standard Document Term Matrix
```{r DTM}
set.seed(1)
# Standard Document Term Matrix
### Classifiers - NB, SVMRadial, SVMLinear
dtm.eval.nb = crossValidate(document_term_matrix, y, 'nb')
kable(dtm.eval.nb)
dtm.eval.svmradial = crossValidate(document_term_matrix, y, 'svmradial')
kable(dtm.eval.svmradial)
dtm.eval.svmlinear = crossValidate(document_term_matrix, y, 'svmlinear')
kable(dtm.eval.svmlinear)
```

### TFIDF Document Term Matrix
```{r TFIDF}
set.seed(1)
# TFIDF Weighted Document Term Matrix, use y.tfidf
### Classifiers - NB, SVMRadial, SVMLinear
tfidf.eval.nb = crossValidate(dtm.tfidf, y.tfidf, 'nb')
kable(tfidf.eval.nb)
tfidf.eval.svmradial = crossValidate(dtm.tfidf, y.tfidf, 'svmradial')
kable(tfidf.eval.svmradial)
tfidf.eval.svmlinear = crossValidate(dtm.tfidf, y.tfidf, 'svmlinear')
kable(tfidf.eval.svmlinear)
```

### LSA DTM
```{r LSA}
set.seed(1)
# LSA Document Term Matrix w/10 features, use y.tfidf
lsa.eval.nb = crossValidate(dtm.lsa.10, y.tfidf, 'nb')
kable(lsa.eval.nb)
lsa.eval.svmradial = crossValidate(dtm.lsa.10, y.tfidf, 'svmradial')
kable(lsa.eval.svmradial)
lsa.eval.svmlinear = crossValidate(dtm.lsa.10, y.tfidf, 'svmlinear')
kable(lsa.eval.svmlinear)
```

### PCA DTM
```{r PCA}
set.seed(1)
# PCA Document Term Matrix
### Classifiers - NB, SVMRadial, SVMLinear
pca.eval.nb = crossValidate(dtm.pca, y, 'nb')
kable(pca.eval.nb)
pca.eval.svmradial = crossValidate(dtm.pca, y, 'svmradial')
kable(pca.eval.svmradial)
pca.eval.svmlinear = crossValidate(dtm.pca, y, 'svmlinear')
kable(pca.eval.svmlinear)
```

### Save 3 Top Models
##### Top Models:  dtm.eval.svmradial (80.55%), dtm.eval.svmlinear (75.24%), pca.eval.svmlinear (75.24%)
```{r }
set.seed(1)
# Get models based of of mean F1 score
model1 = model.fit(as.matrix(document_term_matrix), y, 'svmradial')
model2 = model.fit(dtm.pca, y.tfidf, 'svmlinear')
model3 = model.fit(dtm.pca, y, 'svmlinear')

## Save with 
saveRDS(model1, "./polimodel1.rds")
saveRDS(model2, "./polimodel2.rds")
saveRDS(model3, "./polimodel3.rds")
```




